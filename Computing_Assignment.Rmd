---
title: "Computer Assignment 1"
author:
- Uppsala University
- Department of Statistics
- "Course: Econometrics, Fall 2019"
- "Author: Claes Kock, Mayara Latrech, Yuchong Wu"
- "Date: 9/10/2019"
header-includes:
 \usepackage{float}
output:
  pdf_document:
    number_sections: yes
---

\newpage

\tableofcontents

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(reshape2)
library(ggplot2)
library(moments)
```

\newpage

# Introduction {-}

The first purpose of this assignment is to get a better idea of the properties of various estimators both from a theoretical and practical perspective. The second is obtaining a deeper understanding of the OLS estimator by changing a model's parameters and seeing how the estimator would behave. 

This assignment consists of five tasks that help us work with different data and models. 

In task one, compare the theoretical assumptions to the practical values of the properties of various estimators. 

Tasks two to four focus more on the properties of the OLS estimator. 

In task five we evaluate the choice of a model to approximate certain data points.

\newpage

# Task 1

In this task we have four different estimators:

1) The OLS estimator:

\begin{equation*}
    \widehat{\beta}_{O L S}=\frac{\sum_{t=1}^{T} Y_{t}t}{\sum_{t=1}^{T} t^{2}}
\end{equation*}

2) The Mean estimator:

\begin{equation*}
    b_{M}=\frac{\bar{Y}}{\bar{t}}
\end{equation*}

3) The End estimator:

\begin{equation*}
    b_{E}=\frac{Y_{T}}{T}
\end{equation*}

4)  The OLS estimator for the model with intercept:

\begin{equation*}
    \widehat{\beta}_{O L S I}=\frac{\sum_{t=1}^{T}\left(Y_{t}-\bar{Y}\right)(t-\bar{t})}{\sum_{t=1}^{T}(t-\bar{t})^{2}}
\end{equation*}

## Task 1A

### Unbiasedness

The following calculations will prove that the estimators are unbiased:

Calc 1):

$$\begin{aligned} E\left(\hat{\beta}_{OLS}\right) &=E\left[\frac{\Sigma Y_{t}t}{\Sigma t^{2}}\right] \\ &=\frac{1}{\Sigma t^{2}} E\left[\Sigma Y_{t} t\right] \\ &=\frac{1}{\Sigma t^{2}} \sum E\left[\begin{array}{l}{Y_{t} t}\end{array}\right] \\ &=\frac{1}{\Sigma t^{2}} \Sigma\left[t^{2} \beta+t E\left(u_{t}\right)\right] \ \ \ \  (Assume \ that \ E(u_{t}) = 0)\\ &=\beta+0 \\ &=\beta \end{aligned}$$

\newpage

Calc 2):

$$\begin{aligned} E\left[b_{M}\right] &=E\left[\frac{\bar{Y}}{\bar{t}}\right] \\ &=\frac{1}{\bar{t}} E[\bar{Y}] \\ &=\frac{1}{\bar{t}} E\left[\frac{1}{T} \Sigma\left(\beta t+u_{t}\right)\right] \\ &={\frac{1}{\bar{t}}} \frac{1}{T} \sum E\left(\beta t+u_{t}\right) \\ &=\frac{1}{\bar{t} T} \sum \beta_{t} \\ &=\frac{\beta}{\bar{t} T} \sum{t} \\ &=\beta \end{aligned}$$

Calc 3):

$$\begin{aligned} E\left[b_{E}\right] &=E\left[\frac{Y_{T}}{T}\right] \\ &=\frac{1}{T} E\left[\beta T+u_{T}\right] \\ &=\frac{1}{T} \beta T+\frac{1}{T} E\left[u_{T}\right] \\ &=\beta \end{aligned}$$

Calc 4):

$$\begin{aligned} E\left[\hat{\beta}_{OLSI}\right] &=E\left[\frac{\Sigma\left(Y_{t}-\bar{Y}\right)(t-\bar{t})}{\sum(t-\bar{t})^{2}}\right] \\ &=\frac{1}{\sum(t-\bar{t})^{2}} E\left[\sum_{t=1}^{T}\left[\beta t+u_{t}-\frac{1}{T} \sum_{s=1}^{T}\left(\beta_{s} s+u_{s}\right)\right][t-\bar{t}]\right] \\ &=\frac{1}{\sum(t-\bar{t})^{2}} \sum_{t=1}^{T}\left[\beta t+E\left(u_{t}\right)-\frac{1}{T} \sum_{s=1}^{T}\left(\beta_{s} s+E\left[u_{s}\right]\right)\right][t-\bar{t}] \\ &=\frac{1}{\sum(t-\bar{t})^{2}} \sum(t \beta-\beta \bar{t})(t-\bar{t}) \\ &=\frac{\beta}{\sum(t-\bar{t})^{2}} \sum(t -\bar{t})(t-\bar{t}) \\ &=\beta \end{aligned}$$

\newpage

### Variance

The following calculations are the variances of the estimators:

Calc 1):

\begin{align*} 
Var(\hat{\beta_{OLS}}) &=  Var(\frac{\sum_{t=1}^{T} Y_{t} t }{\sum_{t=1}^{T} t^2}) \\ 
 &=  \frac{1}{(\sum_{t=1}^{T} t^2)^2} Var(\sum_{t=1}^{T} \beta t^2 + u_{t} t) \ \ \ \ we \ assume \ that \ Cov(u_i, u_j) = 0 \ (i\neq j), \ then:\\
 &= \frac{1}{(\sum_{t=1}^{T} t^2)^2} \sum_{t=1}^{T} [Var(\beta t^2 + u_{t} t) + 0]\\
 &= \frac{1}{(\sum_{t=1}^{T} t^2)^2} \sum_{t=1}^{T} (t^2 Var(u_{t}))\\
 &= \frac{1}{(\sum_{t=1}^{T} t^2)^2} \sum_{t=1}^{T} (t^2 \sigma^2)\\
 &= \sigma^2 \frac{1}{(\sum_{t=1}^{T} t^2)^2} \sum_{t=1}^{T} t^2\\
 &= \frac{\sigma^2}{\sum_{t=1}^{T} t^2}
\end{align*}

Calc 2):

\begin{align*}
    Var(b_{M}) &= Var(\frac{\bar{Y}}{\bar{t}})\\
    &= \frac{1}{\bar{t}^2} Var(\frac{1}{T} \sum_{t=1}^{T} \beta t + u_{t}) \ \ \ \ we \ assume \ that \ Cov(u_i, u_j) = 0 \ (i\neq j), \ then:\\
    &= \frac{1}{\bar{t}^2} \frac{1}{T^2} \sum_{t=1}^{T} [Var(\beta t + u_{t}) + 0]\\
    &= \frac{1}{\bar{t}^2} \frac{1}{T^2} T \sigma^2\\
    &= \frac{\sigma^2}{T\bar{t}^2}
\end{align*}

Calc 3):

\begin{align*}
    Var(b_{E}) &= Var(\frac{Y_{T}}{T})\\
    &= \frac{1}{T^2} Var(\beta T + u_{T})\\
    &= \frac{\sigma^2}{T^2}\\
\end{align*}

Calc 4):

\begin{align*}
    Var( \hat{\beta}_{OLSI}) &= Var(\frac{\sum_{t=1}^{T} (Y_{t}-\bar{T})(t-\bar{t})}{\sum_{t=1}^{T}(t-\bar{t})^2}\\
    &= \frac{1}{(\sum_{t=1}^{T}(t-\bar{t})^2)^2)} Var(\sum_{t=1}^{T} (Y_{t}-\bar{Y})(t-\bar{t})\\
    &= \frac{1}{(\sum_{t=1}^{T}(t-\bar{t})^2)^2)} Var( \sum_{t=1}^{T} \beta t^2 + u_{t}t-\bar{Y}t- \beta \bar{t}t-\bar{t}u_{t}+\bar{t}\bar{Y})\\
    &= \frac{1}{(\sum_{t=1}^{T}(t-\bar{t})^2)^2)} \sum_{t=1}^{T} Var(\beta t^2 + u_{t}t-\bar{Y}t- \beta \bar{t}t-\bar{t}u_{t}+\bar{t}\bar{Y}) \\
    & we \ assume \ that \ Cov(u_i, u_j) = 0 \ (i\neq j), \ then: \\
    &= \frac{1}{(\sum_{t=1}^{T}(t-\bar{t})^2)^2} \sum_{t=1}^{T} Var(tu_{t}-\bar{t}u_{t})\\
    &= \frac{1}{(\sum_{t=1}^{T}(t-\bar{t})^2)^2} \sum_{t=1}^{T} Var(u_{t}(t-\bar{t}))\\
    &= \frac{1}{(\sum_{t=1}^{T}(t-\bar{t})^2)^2} \sigma^2 \sum_{t=1}^{T} (t-\bar{t})^2\\
    &= \frac{\sigma^2}{\sum_{t=1}^{T}(t-\bar{t})^2}
\end{align*}


## Task 1B

This section contains a simulation study for T = 40, with 2000 replications.
*We are using the r seed (931229) to generate our sample data.*

```{r echo=FALSE, out.width='70%', fig.align='center', fig.show='hold', fig.cap = "Sampling Distribution for Four Estimators (T = 40)"}
# Compares four estimators of beta2 in single linear regression
# By Johan Lyhagen, Uppsala University, 2005-02-22
# Translated to R by Lukas Arnroth, Uppsala University, 2016-06-12

### Simulation program starts

# set size of sample by assigning a value to n which
# will be used later in the program
n <- 40

# Number of replications
reps <-2000


# Set seed of random number generator
# Set the seed to the birthdate of one of the group members!

seed_group <- 931229
set.seed(seed_group)

# Parameters of the regression
beta_1 <- 0
beta_2 <- 1
sigma_2 <- 1 # Note that this is the variance, not the standard error

# Generate t, which is the independent variable
# (wrote x as in the eviews code)
x <- 1:n # 1, ..., 40

xx = x*x # 1^2, ..., 40^2
sumxx<- sum(xx) # sum of squares of x


# Create matrix to store the results from the simulations
# We want one row (nrow) per repetition and one column for 
# each estimator. 
out <- matrix(nrow = reps, ncol = 4, dimnames = list(NULL, c("OLS", "Mean", "End", "OLS_inter")))

# generate Y and residuals
set.seed(seed_group) 
for(i in 1:reps){
  # residuals
  u = rnorm(n)*sqrt(sigma_2)
  # data generating process of y, the dependent variable
  y = beta_1 + beta_2*x + u
  yx = y*x
  # estimator 1, replication i
  out[i, 1] <- sum(yx)/sumxx
  # estimator 2, replication i
  out[i, 2] <- mean(y)/mean(x)
  # estimator 3, replication i
  # n'th Y and T 
  out[i, 3] <- y[n]/n
  # estimator 4, replication i
  out[i, 4] <- cov(y, x)/var(x)
}

# The melt function is from the reshape2 package. It makes the 
# matrix into a data frame which is more suitable for plotting in ggplot
data <- melt(out, value.name = "Estimate")
# calling 'data' you notice that the matrix has been transformed into a data frame
# with 3 variables, first one is the time(rowname), second is estimator name (column name) 
# and third is the estimate (elements of the out matrix)

# change the column names to be more intuitive
colnames(data) <- c("row Id", "Estimator", "Estimate")

# density plot
ggplot(data) + 
  stat_density(aes(x = Estimate, color = Estimator), 
               geom = "line", position = "identity", size = 1) +
  theme_light() + 
  scale_colour_discrete("Estimator") + 
  xlab(NULL)
```

From a visual perspective, we can draw some conclusions regarding our four estimators. All four estimators seem to be unbiased, i.e. centered around the expected value $\beta=1$, which is also supported by our calculations from the previous subtask.


The OLS estimator seems to have the lowest variance and the Mean estimator seems to have the second lowest variance. The End estimator has the largest variance, which again is supported by the theoretical calculations.


Our plots are not skewed, but some plots have "dents" at the peak of the graph. This can be explained by the random sample generated by our seed - when we changed our seed the dents also changed to different places.

```{r echo=FALSE}
# for kurtosis and skewnewss
table = data.frame(Estimators = c("OLS", "Mean", "End", "OLS_inter"),
                   T = rep(40, 4),
                   Mean = apply(out, 2, mean),
                   Median = apply(out, 2, median),
                   Variance = apply(out, 2, var),
                   Skewness = apply(out, 2, skewness),
                   Kurtosis = apply(out, 2, kurtosis))
rownames(table) = c()
kable(table, caption = "Descriptives of the sampling distribution for four estimators (T = 40)", digits = c(NA, 2, 2, 2, 8, 2, 2))
```

Looking at the table, we see that the mean is approximately equal to the expected value for the different estimators. The variances are also very close to 0 but there are some small differences between the different variances. We can conclude that the OLS estimator is the best one out of the four unbiased estimators in the linear regression, since it seems that its variance is the smallest. This is reasonable since graphically the density of the OLS estimator is the most narrow.

The "End" estimator is the **largest** variance out of all the estimators. This can be explained by the fact that this estimator only measures point T. While the other estimators take into consideration all the data points we have, the End estimator only measures one point.

**None** of the sampling distributions seem to be skewed since the values representing skewness are very low. 

## Task 1C

*Task C is a repeat of Task 1B with a sample size of 400 instead of 40. The same seed and amount of replications apply.*

```{r echo=FALSE, out.width='70%', fig.align='center', fig.show='hold', fig.cap = "\\label{fig:mwe-plot}Sampling Distribution for Four Estimators (T = 400)"}
# Compares four estimators of beta2 in single linear regression
# By Johan Lyhagen, Uppsala University, 2005-02-22
# Translated to R by Lukas Arnroth, Uppsala University, 2016-06-12

### Simulation program starts

# set size of sample by assigning a value to n which
# will be used later in the program
n <- 400

# Number of replications
reps <-2000


# Set seed of random number generator
# Set the seed to the birthdate of one of the group members!

seed_group <- 931229
set.seed(seed_group)

# Parameters of the regression
beta_1 <- 0
beta_2 <- 1
sigma_2 <- 1 # Note that this is the variance, not the standard error

# Generate t, which is the independent variable
# (wrote x as in the eviews code)
x <- 1:n # 1, ..., 40

xx = x*x # 1^2, ..., 40^2
sumxx<- sum(xx) # sum of squares of x


# Create matrix to store the results from the simulations
# We want one row (nrow) per repetition and one column for 
# each estimator. 
out2 <- matrix(nrow = reps, ncol = 4, dimnames = list(NULL, c("OLS", "Mean", "End", "OLS_inter")))

# generate Y and residuals
set.seed(seed_group) 
for(i in 1:reps){
  # residuals
  u = rnorm(n)*sqrt(sigma_2)
  # data generating process of y, the dependent variable
  y = beta_1 + beta_2*x + u
  yx = y*x
  # estimator 1, replication i
  out2[i, 1] <- sum(yx)/sumxx
  # estimator 2, replication i
  out2[i, 2] <- mean(y)/mean(x)
  # estimator 3, replication i
  # n'th Y and T 
  out2[i, 3] <- y[n]/n
  # estimator 4, replication i
  out2[i, 4] <- cov(y, x)/var(x)
}

# The melt function is from the reshape2 package. It makes the 
# matrix into a data frame which is more suitable for plotting in ggplot
data <- melt(out2, value.name = "Estimate")
# calling 'data' you notice that the matrix has been transformed into a data frame
# with 3 variables, first one is the time(rowname), second is estimator name (column name) 
# and third is the estimate (elements of the out2 matrix)

# change the column names to be more intuitive
colnames(data) <- c("row Id", "Estimator", "Estimate")

# density plot
ggplot(data) + 
  stat_density(aes(x = Estimate, color = Estimator), 
               geom = "line", position = "identity", size = 1) +
  theme_light() + 
  scale_colour_discrete("Estimator") + 
  xlab(NULL)
```

From a visual perspective we can tell that the estimators are still unbiased since they are all centered around the true value of $\beta=1$. However the variances are now smaller, but the size of the variances compared to each other still have the same ordering as in 1B, i.e. OLS still has the smallest variance, followed by "Mean" estimator, etc. The difference of the variances are now much smaller compared to when the sample size was 40. This could mean that a further increase in sample size will lead to a smaller difference between the variances of the estimators.


Our plots are not skewed, but some plots have "dents" at the peak of the graph. This can be explained by the random sample generated by our seed - when we changed our seed the dents also changed to different places.
 
```{r echo=FALSE}
# for kurtosis and skewnewss
table2 = data.frame(Estimators = c("OLS", "Mean", "End", "OLS_inter"),
                   T = rep(400, 4),
                   Mean = apply(out2, 2, mean),
                   Median = apply(out2, 2, median),
                   Variance = apply(out2, 2, var),
                   Skewness = apply(out2, 2, skewness),
                   Kurtosis = apply(out2, 2, kurtosis))
rownames(table2) = c()
kable(table2, caption = "Descriptives of the sampling distribution for four estimators (T = 400)", digits = c(NA, 2, 2, 2, 8, 2, 2))
```

We can again see that the mean of all the estimators are **still** approximately equal to their expected values. The variances are also similar to the previous simulation, but far smaller. We can conclude that the OLS estimator is still the best one out of the four unbiased estimators in the linear regression, since it again seems like its variance is the smallest. This is reasonable since graphically the density of the OLS estimator is the most narrow, although the difference between all of the variances are far smaller.

The End estimator has the largest variance out of all the estimators. This can be explained by the same fact as in 1B.

Nothing has changed regarding the skewness of the estimators compared to the first simulation.

## Task 1D

```{r echo=FALSE}
table = rbind(table, table2)
kable(table, caption = "Compare the results between the two simulation studies", digits = c(NA, 2, 2, 2, 8, 2, 2))
```


Since the variance of all of the estimators decreased when we increased the sample size from 40 to 400, we suspect that if we keep increasing the sample size to 4000, and then to 40000, the variances will get smaller and smaller. The shape of the distribution will become more narrow. We suspect that these changes are caused by the fact that the estimators are actually consistent; meaning that their variances will converge to 0 when we increase the sample size.

\newpage

# Task 2

## Task 2A

We start with generating our two sets of 40 observations each, one for $\sigma = 1$ and the other $\sigma = 10$. The estimations of the model's parameters can be found in the table below. 

```{r echo=FALSE}
# Compares four estimators of beta2 in single linear regression
# By Johan Lyhagen, Uppsala University, 2005-02-22
# Translated to R by Lukas Arnroth, Uppsala University, 2016-06-12

library(reshape2)
library(ggplot2)

### Simulation program starts

# set size of sample by assigning a value to n which
# will be used later in the program
n <- 40

# Number of replications
reps <-2000

# Set seed of random number generator
# IMPORTANT!
# Set the seed to the birthdate of one of the group members!
# For mee it would look like this:
seed_group <- 931229
set.seed(seed_group) 

# so in your code, change the 911021 to the birthdate of
# any one of your group members.

# Parameters of the regression
beta_1 <- 10
beta_2 <- 1
sigma_2 <- 1 # Note that this is the variance, not the standard error

# Generate t, which is the independent variable
# (wrote x as in the eviews code)
x <- 1:n # 1, ..., 40

xx = x*x # 1^2, ..., 40^2
sumxx<- sum(xx) # sum of squares of x


# Create matrix to store the results from the simulations
# We want one row (nrow) per repetition and one column for 
# each estimator. 
out <- matrix(nrow = reps, ncol = 4, dimnames = list(NULL, c("OLS", "Mean", "End", "OLS_inter")))

# generate Y and residuals
set.seed(seed_group) 
for(i in 1:reps){
  # residuals
  u = rnorm(n)*sqrt(sigma_2)
  # data generating process of y, the dependent variable
  y = beta_1 + beta_2*x + u
  yx = y*x
  # estimator 1, replication i
  out[i, 1] <- sum(yx)/sumxx
  # estimator 2, replication i
  out[i, 2] <- mean(y)/mean(x)
  # estimator 3, replication i
  # n'th Y and T 
  out[i, 3] <- y[n]/n
  # estimator 4, replication i
  out[i, 4] <- cov(y, x)/var(x)
}

# The melt function is from the reshape2 package. It makes the 
# matrix into a data frame which is more suitable for plotting in ggplot
data <- melt(out, value.name = "Estimate")
# calling 'data' you notice that the matrix has been transformed into a data frame
# with 3 variables, first one is the time(rowname), second is estimator name (column name) 
# and third is the estimate (elements of the out matrix)

# change the column names to be more intuitive
colnames(data) <- c("row Id", "Estimator", "Estimate")


### 2A
# in evaluating a OLS model we will use the 'lm' (linear model) function.
# calling ?lm() gives the R documentation for this function
# the requirement in this case is to supply the lm with its y and x
# note in the documentation that the formula uses '~' instead of '='
# as '=' is reserved in the R syntax as assignment (same as '<-')
# Look into the global environment and consider how to do the estimate
# Remember to save the estimate in the global environment by assigning 
# the lm() call to an object as follows:

# estimate <- lm(a ~ b)

# When evaluating the model, use the summary() function on the linear model object
# you saved in the global environment:

# summary(estimate)

# solution
estimate_1 <- lm(y ~ x) # ignore data argument of lm as we're calling two seperate vectors
# evaluate by plots
# begin by combining necessary results into one data frame
estimate_data_1 <- data.frame(y_hat = estimate_1$fitted.values,
                            resid = estimate_1$residuals,
                            y = y,
                            t = x)






# Compares four estimators of beta2 in single linear regression
# By Johan Lyhagen, Uppsala University, 2005-02-22
# Translated to R by Lukas Arnroth, Uppsala University, 2016-06-12

library(reshape2)
library(ggplot2)

### Simulation program starts

# set size of sample by assigning a value to n which
# will be used later in the program
n <- 40

# Number of replications
reps <-2000

# Set seed of random number generator
# IMPORTANT!
# Set the seed to the birthdate of one of the group members!
# For mee it would look like this:
seed_group <- 931229
set.seed(seed_group) 

# so in your code, change the 911021 to the birthdate of
# any one of your group members.

# Parameters of the regression
beta_1 <- 10
beta_2 <- 1
sigma_2 <- 100 # Note that this is the variance, not the standard error

# Generate t, which is the independent variable
# (wrote x as in the eviews code)
x <- 1:n # 1, ..., 40

xx = x*x # 1^2, ..., 40^2
sumxx<- sum(xx) # sum of squares of x


# Create matrix to store the results from the simulations
# We want one row (nrow) per repetition and one column for 
# each estimator. 
out <- matrix(nrow = reps, ncol = 4, dimnames = list(NULL, c("OLS", "Mean", "End", "OLS_inter")))

# generate Y and residuals
set.seed(seed_group) 
for(i in 1:reps){
  # residuals
  u = rnorm(n)*sqrt(sigma_2)
  # data generating process of y, the dependent variable
  y = beta_1 + beta_2*x + u
  yx = y*x
  # estimator 1, replication i
  out[i, 1] <- sum(yx)/sumxx
  # estimator 2, replication i
  out[i, 2] <- mean(y)/mean(x)
  # estimator 3, replication i
  # n'th Y and T 
  out[i, 3] <- y[n]/n
  # estimator 4, replication i
  out[i, 4] <- cov(y, x)/var(x)
}

# The melt function is from the reshape2 package. It makes the 
# matrix into a data frame which is more suitable for plotting in ggplot
data <- melt(out, value.name = "Estimate")
# calling 'data' you notice that the matrix has been transformed into a data frame
# with 3 variables, first one is the time(rowname), second is estimator name (column name) 
# and third is the estimate (elements of the out matrix)

# change the column names to be more intuitive
colnames(data) <- c("row Id", "Estimator", "Estimate")

### 2A
# in evaluating a OLS model we will use the 'lm' (linear model) function.
# calling ?lm() gives the R documentation for this function
# the requirement in this case is to supply the lm with its y and x
# note in the documentation that the formula uses '~' instead of '='
# as '=' is reserved in the R syntax as assignment (same as '<-')
# Look into the global environment and consider how to do the estimate
# Remember to save the estimate in the global environment by assigning 
# the lm() call to an object as follows:

# estimate <- lm(a ~ b)

# When evaluating the model, use the summary() function on the linear model object
# you saved in the global environment:

# summary(estimate)

# solution
estimate_2 <- lm(y ~ x) # ignore data argument of lm as we're calling two seperate vectors
# evaluate by plots
# begin by combining necessary results into one data frame
estimate_data_2 <- data.frame(y_hat = estimate_2$fitted.values,
                            resid = estimate_2$residuals,
                            y = y,
                            t = x)

summa_1 = summary(estimate_1)$coefficients
summa_1 = summa_1[, 1:2]
rownames(summa_1) = c("$\\beta_1 \\ (\\sigma = 1)$", "$\\beta_2 \\ (\\sigma = 1)$")
colnames(summa_1) = c("Estimate", "Variance")
summa_1[, 2] = summa_1[, 2]^2

summa_2 = summary(estimate_2)$coefficients
summa_2 = summa_2[, 1:2]
rownames(summa_2) = c("$\\beta_1 \\ (\\sigma = 10)$", "$\\beta_2 \\ (\\sigma = 10)$")
colnames(summa_2) = c("Estimate", "Variance")
summa_2[, 2] = summa_2[, 2]^2

summa = rbind(summa_1, summa_2)

kable(summa, caption = "Estimations and variances for regression coefficients", digit = 2)
```

We notice that the intercept and slope slightly changed while the variances drastically increased when the standard deviation increased. 

1) When we increase the standard deviation from 1 to 10, we notice that the intercept, i.e. $\hat{\beta_{1}}$, has increased slightly. The change in the variable $\hat{\beta_{1}}$ is much smaller than the change in the estimator of the variance, $Var(\hat{\beta_{1}})$, which has drastically changed. This is due to the change of the standard deviation. Thus, we can observe that a change in standard deviation affects the variance to a greater extent than the estimators of the variables. We assume that if we were to increase $\sigma = 20$, $Var(\hat{\beta_{1}})$ would increase even further, and $\hat{\beta_{1}}$ will change.

2) When we increase the standard deviation from 1 to 10, we notice that the intercept, i.e. $\hat{\beta_{2}}$, has decreased slightly instead of increased. The change in the variable $\hat{\beta_{2}}$ is still much smaller than the change in the estimator of the variance. We can still observe that a change in standard deviation affects the variance to a greater extent than the estimators of the variables. We assume that if we were to increase $\sigma = 20$, $Var(\hat{\beta_{2}})$ would increase even further, and $\hat{\beta_{1}}$ will change.


## Task 2B

*The detailed result of the regression can be seen in the appendix*

Calculate (numerically) for the model with $\sigma$ = 10 in Task 2A:

```{r echo=FALSE}
kable(head(estimate_data_2), caption = "Results For the Regression when $sd = 10$ (6 out of 40 heads)", digit = 3)
```

\newpage

```{r echo=FALSE}
cor1 = cor(estimate_data_2$t, estimate_data_2$resid)
```

The correlation between $t$ and $\hat{u_{t}}$ is `r cor1`.

```{r echo=FALSE}
cor2 = cor(estimate_data_2$y_hat, estimate_data_2$resid)
```

The correlation between $\hat{Y_{t}}$ and $\hat{u_{t}}$ is `r cor2`.

```{r echo=FALSE}
cor3 = cor(estimate_data_2$y, estimate_data_2$resid)
```

The correlation between $Y_{t}$ and $\hat{u_{t}}$ is `r cor3`.

```{r echo=FALSE}
cor4 = cor(estimate_data_2$y, estimate_data_2$t)
```

The correlation between $Y_{t}$ and $t$ is `r cor4`.

```{r echo=FALSE}
tab = data.frame(Correlation_between = c("$t$ and $\\hat{u_{t}}$", "$\\hat{Y_{t}}$ and $\\hat{u_{t}}$", "$Y_{t}$ and $\\hat{u_{t}}$", "$Y_{t}$ and $t$"),
                   Theoretical_value = c(0, 0, NA, NA),
                   Simulated_value = c(cor1, cor2, cor3, cor4)
                   )
names(tab) = c("Correlation between", "Theoretical values", "Simulated values") 
kable(tab, caption = "Comparison between Theoretical values and Simulated values", digits = 3)
Correlation_between = c("$t$ and $\\hat{u_{t}}$", "$\\hat{Y_{t}}$ and $\\hat{u_{t}}$", "$Y_{t}$ and $\\hat{u_{t}}$", "$Y_{t}$ and $t$")
```

Theoretically, there should **not** be a correlation between the residual ($\hat{u}$) and the independent variable ($t$), which has been proven in the OLS linear regression. $\hat{Y_{t}}$ and $t$ should **not** be correlated because $\hat{Y_{t}}$ is a function of $t$. The first two correlations in this task are effectively zero, and thus match our expectations. The third and fourth correlation from the result of simulation are reasonable since the correlations do exist and we can never calculate them before we make a simulation. 

\newpage

# Task 3

## 1

The plots for the two weights and the unweighted plot seem to look the same at a glance, however, they are scaled differently for the Y axis. This is natural because the weight only applies to the Y variable, so only the Y axis should be affected. If we scale all the plots to the same Y axis we should see much greater differences of the slope. The slope for w1 = 10 would be greater, and w1 = 0.1 would be flatter.

```{r fig.align='center', echo=FALSE, fig.cap="Plots of the actual and fitted for the rescaled and unscaled cases", fig.show='hold'}
w_x = 1
w_y = 1

# rescale x & y
x_resc = x * w_x
y_resc = y * w_y

# fit the linear model. Note that no data argument is
# used in this call as the lm() function is supplied
# with two seperate vectors
resc_fit_1 <- lm(y_resc ~ x_resc)

# Next is the plotting. Focus on just using this and getting out
# the result rather than making sense of the code. It's good if 
# you have a general understanding of what is going on it, but you
# are not expected to create this on your own!

# split the plot window into two rows and one column
par(mfcol = c(2,3))

### plot in the first position
# represent the actualy values as dots. Note that ylab and xlab
# will change when you change the weights!
plot(y = y_resc, x = x_resc, col = "blue", 
     ylab = paste(w_y, "* Y"), xlab = paste(w_x, "* X"), 
     main = "Model Evaluation", ylim = c(-5, max(y))*w_y) 
# add line from the linear model fit
abline(resc_fit_1, col = "red") 

# plot in the second position
plot(y = resc_fit_1$resid, x = x_resc, ylab = "Resid", xlab = paste(w_x, "* X"))
abline(a = 0, b = 0, col = "gray", lty = 2)
lines(y = resc_fit_1$resid, x = x_resc)

##################################

w_x = 1
w_y = 10

# rescale x & y
x_resc = x * w_x
y_resc = y * w_y

# fit the linear model. Note that no data argument is
# used in this call as the lm() function is supplied
# with two seperate vectors
resc_fit_2 <- lm(y_resc ~ x_resc)

# Next is the plotting. Focus on just using this and getting out
# the result rather than making sense of the code. It's good if 
# you have a general understanding of what is going on it, but you
# are not expected to create this on your own!


### plot in the first position
# represent the actualy values as dots. Note that ylab and xlab
# will change when you change the weights!
plot(y = y_resc, x = x_resc, col = "blue", 
     ylab = paste(10, "* Y"), xlab = paste(w_x, "* X"), 
     main = "Model Evaluation", ylim = c(-5, max(y))*w_y) 
# add line from the linear model fit
abline(resc_fit_2, col = "red") 

# plot in the second position
plot(y = resc_fit_2$resid, x = x_resc, ylab = "Resid", xlab = paste(w_x, "* X"))
abline(a = 0, b = 0, col = "gray", lty = 2)
lines(y = resc_fit_2$resid, x = x_resc)

##################################

w_x = 1
w_y = 0.1

# rescale x & y
x_resc = x * w_x
y_resc = y * w_y

# fit the linear model. Note that no data argument is
# used in this call as the lm() function is supplied
# with two seperate vectors
resc_fit_3 <- lm(y_resc ~ x_resc)

# Next is the plotting. Focus on just using this and getting out
# the result rather than making sense of the code. It's good if 
# you have a general understanding of what is going on it, but you
# are not expected to create this on your own!


### plot in the first position
# represent the actualy values as dots. Note that ylab and xlab
# will change when you change the weights!
plot(y = y_resc, x = x_resc, col = "blue", 
     ylab = paste(0.1, "* Y"), xlab = paste(w_x, "* X"), 
     main = "Model Evaluation", ylim = c(-5, max(y))*w_y) 
# add line from the linear model fit
abline(resc_fit_3, col = "red") 

# plot in the second position
plot(y = resc_fit_3$resid, x = x_resc, ylab = "Resid", xlab = paste(w_x, "* X"))
abline(a = 0, b = 0, col = "gray", lty = 2)
lines(y = resc_fit_3$resid, x = x_resc)
```

## 2

```{r echo=FALSE}
summa_1 = summary(resc_fit_1)$coefficients
summa_1 = summa_1[, 1:3]
rownames(summa_1) = c("$\\beta_1 \\ (w = 1)$", "$\\beta_2 \\ (w = 1)$")
colnames(summa_1) = c("Estimate", "Variance", "T-test")
summa_1[, 2] = summa_1[, 2]^2

summa_2 = summary(resc_fit_2)$coefficients
summa_2 = summa_2[, 1:3]
rownames(summa_2) = c("$\\beta_1 \\ (w = 10)$", "$\\beta_2 \\ (w = 10)$")
colnames(summa_2) = c("Estimate", "Variance", "T-test")
summa_2[, 2] = summa_2[, 2]^2

summa_3 = summary(resc_fit_3)$coefficients
summa_3 = summa_3[, 1:3]
rownames(summa_3) = c("$\\beta_1 \\ (w = 0.1)$", "$\\beta_2 \\ (w = 0.1)$")
colnames(summa_3) = c("Estimate", "Variance", "T-test")
summa_3[, 2] = summa_3[, 2]^2


summa = rbind(summa_1, summa_2, summa_3)

kable(summa, caption = "Estimations and variances for regression coefficients", digit = 2)
```

## 3

When we apply the weight to the variable Y, $\beta_{1}$ and $Var(\beta_{1})$ change accordingly i.e. the new $\beta_{1}$ becomes the old $\beta_{1}$ times $w_{1}$ and the new variance becomes the old variance multiplied by $w_{1}^2$.

$\hat Y_{t}= \hat\beta_{1}'+ \hat\beta_{2}'t+ \hat\mu_{t}$ where $\beta_{1}'=10$ and $\beta_{2}'=1$

$Y_{t}^{***}= \beta_{1}+ \beta_{2}t+ \mu_{t}$

$Y_{t}^{***}= \omega_{1}Y_{t}= \omega_{1}\beta_{1}'+\omega_{1}\beta_{2}'t+\omega_{1}\mu_{t}$

So, $\beta_{1}=\omega_{1}\beta_{1}'$ and $\beta_{2}=\omega_{1}\beta_{2}'$.

We also get, $Var(\beta_{1})=Var(\omega_{1}\beta_{1}')=\omega_{1}^{2}Var(\beta_{1}')$ and $Var(\beta_{2})=Var(\omega_{1}\beta_{2}')=\omega_{1}^{2}Var(\beta_{2}')$.

This can be seen in our table, as the estimate of $\hat\beta_{1}w_{1} = 10.92$, but the same beta times the weight of 10, $\hat\beta_{1}w_{10} = 109.20$. This means that the only difference between the estimates are the weights we put on them. Same thing can be seen with the variances.
The t-statistic does not change. Theoretically, when calculating the t-value we would multiply and divide by $w_{1}$, i.e 

$t_{value}^{****}= \frac{Var(\hat\beta_1)-\beta_1}{\sigma_{\hat\beta_1}}=\frac{w_1\hat\beta_1'-w_1\beta_1'}{w_1\sigma_{\hat\beta_1'}}=t_{value}$

Hence, we can simplify and thus not change the t-value. Intuitively, the t-value measures the size of the difference relative to the variation the data so it would not get affected by weighting the data. We can see the proof in our table by observing that all the t-tests for $\beta_{1}$ are the same number. 

## 4

Similarly to the previous observation, when we apply our weight we multiply the estimates with our weight. $w_{1}\hat{\beta_{2}} = w_{1}\hat{\beta_{2}}$. We can see that $w_{1}\hat{\beta_{2}} = 0.97$ and $w_{1} = 10$ is equal to 9.67. The variance also follows this pattern but with larger differences due to the weights being squared.
Theoretically, when calculating the t-value we would multiply and divide by $w_{1}$, i.e $t_{value} = w_{1}t_{value}/w_{1}$ Hence, we can simplify and thus not change the t-value. Hence, it will be simplified and thus not change the t-value. Intuitively, the t-value measures the size of the difference relative to the variation the data so it would not get affected by weighting the data. We can see in our table that the t-value doesn't change. 


\newpage

# Task 4

## 1

The plots for the two weights and the unweighted plot seem to look the same at a glance, however, they are scaled differently for the X axis. This is natural because the weight only applies to the X variable, so only the X axis should be affected. If we scale all the plots to the same X axis we should see much greater differences of the slope. The slope for w2 = 10 would be flatter, and w2 = 0.1 would be greater.

```{r fig.align='center', echo=FALSE, fig.cap="Plots of the actual and fitted for the rescaled and unscaled cases", fig.show='hold'}
w_x = 1
w_y = 1

# rescale x & y
x_resc = x * w_x
y_resc = y * w_y

# fit the linear model. Note that no data argument is
# used in this call as the lm() function is supplied
# with two seperate vectors
resc_fit_1 <- lm(y_resc ~ x_resc)

# Next is the plotting. Focus on just using this and getting out
# the result rather than making sense of the code. It's good if 
# you have a general understanding of what is going on it, but you
# are not expected to create this on your own!

# split the plot window into two rows and one column
par(mfcol = c(2,3))

### plot in the first position
# represent the actualy values as dots. Note that ylab and xlab
# will change when you change the weights!
plot(y = y_resc, x = x_resc, col = "blue", 
     ylab = paste(w_y, "* Y"), xlab = paste(w_x, "* X"), 
     main = "Model Evaluation", ylim = c(-5, max(y))*w_y) 
# add line from the linear model fit
abline(resc_fit_1, col = "red") 

# plot in the second position
plot(y = resc_fit_1$resid, x = x_resc, ylab = "Resid", xlab = paste(w_x, "* X"))
abline(a = 0, b = 0, col = "gray", lty = 2)
lines(y = resc_fit_1$resid, x = x_resc)

##################################

w_x = 10
w_y = 1

# rescale x & y
x_resc = x * w_x
y_resc = y * w_y

# fit the linear model. Note that no data argument is
# used in this call as the lm() function is supplied
# with two seperate vectors
resc_fit_2 <- lm(y_resc ~ x_resc)

# Next is the plotting. Focus on just using this and getting out
# the result rather than making sense of the code. It's good if 
# you have a general understanding of what is going on it, but you
# are not expected to create this on your own!


### plot in the first position
# represent the actualy values as dots. Note that ylab and xlab
# will change when you change the weights!
plot(y = y_resc, x = x_resc, col = "blue", 
     ylab = paste(1, "* Y"), xlab = paste(w_x, "* X"), 
     main = "Model Evaluation", ylim = c(-5, max(y))*w_y) 
# add line from the linear model fit
abline(resc_fit_2, col = "red") 

# plot in the second position
plot(y = resc_fit_2$resid, x = x_resc, ylab = "Resid", xlab = paste(w_x, "* X"))
abline(a = 0, b = 0, col = "gray", lty = 2)
lines(y = resc_fit_2$resid, x = x_resc)

##################################

w_x = 0.1
w_y = 1

# rescale x & y
x_resc = x * w_x
y_resc = y * w_y

# fit the linear model. Note that no data argument is
# used in this call as the lm() function is supplied
# with two seperate vectors
resc_fit_3 <- lm(y_resc ~ x_resc)

# Next is the plotting. Focus on just using this and getting out
# the result rather than making sense of the code. It's good if 
# you have a general understanding of what is going on it, but you
# are not expected to create this on your own!


### plot in the first position
# represent the actualy values as dots. Note that ylab and xlab
# will change when you change the weights!
plot(y = y_resc, x = x_resc, col = "blue", 
     ylab = paste(1, "* Y"), xlab = paste(w_x, "* X"), 
     main = "Model Evaluation", ylim = c(-5, max(y))*w_y) 
# add line from the linear model fit
abline(resc_fit_3, col = "red") 

# plot in the second position
plot(y = resc_fit_3$resid, x = x_resc, ylab = "Resid", xlab = paste(w_x, "* X"))
abline(a = 0, b = 0, col = "gray", lty = 2)
lines(y = resc_fit_3$resid, x = x_resc)
```

## 2

```{r echo=FALSE}
summa_1 = summary(resc_fit_1)$coefficients
summa_1 = summa_1[, 1:3]
rownames(summa_1) = c("$\\beta_1 \\ (w = 1)$", "$\\beta_2 \\ (w = 1)$")
colnames(summa_1) = c("Estimate", "Variance", "T-test")
summa_1[, 2] = summa_1[, 2]^2

summa_2 = summary(resc_fit_2)$coefficients
summa_2 = summa_2[, 1:3]
rownames(summa_2) = c("$\\beta_1 \\ (w = 10)$", "$\\beta_2 \\ (w = 10)$")
colnames(summa_2) = c("Estimate", "Variance", "T-test")
summa_2[, 2] = summa_2[, 2]^2

summa_3 = summary(resc_fit_3)$coefficients
summa_3 = summa_3[, 1:3]
rownames(summa_3) = c("$\\beta_1 \\ (w = 0.1)$", "$\\beta_2 \\ (w = 0.1)$")
colnames(summa_3) = c("Estimate", "Variance", "T-test")
summa_3[, 2] = summa_3[, 2]^2


summa = rbind(summa_1, summa_2, summa_3)

kable(summa, caption = "Estimations and variances for regression coefficients", digit = 2)
```

## 3

When we apply the weight to the variable X, $\beta_{1}$ and $Var(\beta_{1})$ do not really change, we are only applying the weight to $\beta_{2}$ in the model so $\beta_{1}$ and $Var(\beta_{1})$ should not change.

$Y_{t}= \beta_{1}'+ \beta_{2}'t+ \mu_{t}$ where $\beta_{1}'=10$ and $\beta_{2}'=1$

$Y_{t}= \beta_{1}+ \beta_{2}X_{t}+ \mu_{t}=\beta_{1}+ \beta_{2}\omega_{2}t+ \mu_{t}$

So, $\beta_{1}=\beta_{1}'$ and $\beta_{2}\omega_{2}=\beta_{2}'$.

Hence, $Var(\beta_{1})=Var(\beta_{1}')$ and $Var(\beta_{2}')=\omega_{2}^2Var(\beta_{2})$.

The t-statistic does not change as well. 

$t_{value}^{****}= \frac{Var(\hat\beta_2)-\beta_2}{\sigma_{\hat\beta_2}}=\frac{w_2\hat\beta_2'-w_2\beta_2'}{w_2\sigma_{\hat\beta_2'}}=t_{value}$

Theoretically, when calculating the t-value we would multiply and divide by $w_{2}$. Hence, it will be simplified and thus not change the t-value. Intuitively, the t-value measures the size of the difference relative to the variation the data so it would not get affected by weighting the data.

## 4

Similarly to the previous observation, when we apply our weight we multiply the estimates with our weight. $w_{2}\hat{\beta_{2}} = w_{2}\hat{\beta_{2}}$. We can see that $w_{2} = 1$ which is in the table 0.97, and $w_{2} = 10$ which is in the table 9.67. It is approximately 10($w_{2} = 1$). The variance also follows this pattern but with larger differences due to the weights being squared.
Theoretically, when calculating the t-value we would multiply and divide by $w_{2}$, i.e $t_{value} = w_{2}t_{value}/w_{2}$ Hence, we can simplify and thus not change the t-value. Hence, it will be simplified and thus not change the t-value. Intuitively, the t-value measures the size of the difference relative to the variation the data so it would not get affected by weighting the data. We can see in our table that the t-value doesn't change.

\newpage

# Task 5

For each of the four data sets, we estimate a simple regression model with intercept using the standard OLS-estimate for intercept and slope:

$$Y_{i}=\beta_{0}+\beta_{1} X_{i} +u \ \ \ \  (i = 1,2,3,4)$$

Then we get the estimated models:

```{r echo=FALSE}
# As a side comment, if you're getting stressed about the number of objects in 
# the global environment you can either use the broom symbol to clear it out or
# 'rm(list = ls())'.

# rm(list = ls())

### Create the vectors which will be combined into a data frame

# The dependent vectors
Y_1 <- c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 
         7.24, 4.26, 10.84, 4.82, 5.68)

Y_2 <- c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10,
         6.13, 3.10, 9.13, 7.26, 4.74)

Y_3 <- c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84,
         6.08, 5.39, 8.15, 6.42, 5.73)

Y_4 <- c(6.58, 5.76, 7.70, 8.84, 8.47, 7.04,
         5.25, 12.5, 5.56, 7.91, 6.89)

# The independent vectors
X_1 <- c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5)
X_2 <- X_1
X_3 <- X_1
X_4 <- c(rep(8, 7), 19, rep(8,3))

# combine it all into a data frame
data_5 <- data.frame(Y1 = Y_1, Y2 = Y_2, Y3 = Y_3, Y4 = Y_4,
                     X1 = X_1, X2 = X_2, X3 = X_3, X4 = X_4)

### Fit the four models
m1 <- lm(Y1 ~ X1, data = data_5)
m2 <- lm(Y2 ~ X2, data = data_5)
m3 <- lm(Y3 ~ X3, data = data_5)
m4 <- lm(Y4 ~ X4, data = data_5)
```
```{r echo=FALSE}
table = rbind(m1$coefficients, m2$coefficients, m3$coefficients, m4$coefficients)
Model = 1:4
table = cbind(Model, table)
colnames(table) = c("Model", "$\\beta_{0}$", "$\\beta_{1}$")
kable(table, caption = "Regression Results for 4 Models", digit = 4)
```

```{r fig.align='center', echo=FALSE, fig.cap="Simple regression models with intercept using the standard OLS-estimate"}
# To present the results we'll split the
# plot window into four cells (two rows 
# and two columns) 
par(mfrow=c(2,2))

# plot the actual Y's
plot(data_5$X1, data_5$Y1, xlab="x", ylab="y",            # This has been changed
     main = "Model Fit X1 & Y1",pch=16, col = "gray")
# add fitted line to the plot
abline(m1, col = "red")


### for x2 & y2
# plot the actual Y's
plot(data_5$X2, data_5$Y2, xlab="x", ylab="y",            # This has been changed
     main = "Model Fit X2 & Y2",pch=16, col = "gray")
# add fitted line to the plot
abline(m2, col = "red")


### for x3 & y3
# plot the actual Y's
plot(data_5$X3, data_5$Y3,  xlab="x", ylab="y",           # This has been changed
     main = "Model Fit X3 & Y3",pch=16, col = "gray")
# add fitted line to the plot
abline(m3, col = "red")

### for x4 & y4
# plot the actual Y's
plot(data_5$X4, data_5$Y4,  xlab="x", ylab="y",           # This has been changed
     main = "Model Fit X4 & Y4",pch=16, col = "gray")
# add fitted line to the plot
abline(m4, col = "red")

```

\begin{table}[!h] \centering 
  \caption{Comparison among 4 models} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lcccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & Y1 & Y2 & Y3 & Y4 \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4)\\ 
\hline \\[-1.8ex] 
 X1 & 0.500$^{***}$ &  &  &  \\ 
  & (0.118) &  &  &  \\ 
  & & & & \\ 
 X2 &  & 0.500$^{***}$ &  &  \\ 
  &  & (0.118) &  &  \\ 
  & & & & \\ 
 X3 &  &  & 0.500$^{***}$ &  \\ 
  &  &  & (0.118) &  \\ 
  & & & & \\ 
 X4 &  &  &  & 0.500$^{***}$ \\ 
  &  &  &  & (0.118) \\ 
  & & & & \\ 
 Constant & 3.000$^{**}$ & 3.001$^{**}$ & 3.002$^{**}$ & 3.000$^{**}$ \\ 
  & (1.125) & (1.125) & (1.124) & (1.123) \\ 
  & & & & \\ 
\hline \\[-1.8ex] 
Observations & 11 & 11 & 11 & 11 \\ 
R$^{2}$ & 0.667 & 0.666 & 0.666 & 0.667 \\ 
Adjusted R$^{2}$ & 0.629 & 0.629 & 0.629 & 0.630 \\ 
Residual Std. Error (df = 9) & 1.237 & 1.237 & 1.236 & 1.235 \\ 
F Statistic (df = 1; 9) & 17.990$^{***}$ & 17.966$^{***}$ & 17.972$^{***}$ & 18.028$^{***}$ \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

For X1 \& Y1, the spread of the data points seem to give the impression of a pattern; thus it seems to be homoscedastic. The linear plot seems to fit the data points to an extent; but there is still some spread, although this spread seems to be loosely focused around the linear function. This one seems to be the one with the most consistent spread if we were to keep the outlier in the third model. It is a much better tool of estimation for the data compared to model two and model four.

The second model, which is X2 \& Y2, seems to be a much poorer fit compared to the first model, as the data points seem to follow a completely different pattern, which we suspect to be a multiple non linear regression. In the second model the data is not centered around the line, instead following a curve. This makes the data a poor fit for a linear estimation. Compared to the the first and third models it is a worse estimatior, but probably a better one than the fourth model, as that model lacks almost any sort of slope.

For the third model, X3 \& Y3, we can see that a linear model is a rather good fit, although one of the data points is an outlier. Due to this outlier the slope of the linear regression doesn't quite match the slope of the data points. If we were to remove the outlier then we would get a better fit for the model. Despite this the data is approximately linear distributed. If we remove the outlier this might become an even better fit than the first model, due to the fact that the data points seem to be very closely grouped together and seem to follow the same path, with the exception of the outlier who pulls the line upwards towards itself, thus causing the line to have a higher slope. Still a much better model than the second or the fourth, as it is still a very good fit.

For the forth model, The linear approximation seems like a poor fit. The data points seem to be concentrated around $X=8$ with only one extreme value of $X=19$ thus do not follow a linear model. We suspect that $X=8$ is an influential point for the data points because the majority of them are concentrated around it. Due to this data, almost all the observations are centered around a single point, which makes this the worst estimator by far, as any sort of slope the model has is solely generated by the outlier $X=19$. The fourth model is thus the worst model by far, as the data is far to clustered around the same value for linear approximation to work well.

\newpage

# Conclusion

In conclusion, we worked with various estimators in this assignment and compared their theoretical properties to their practical properties. We took a deeper dive into the properties of the OLS estimators when we change the parameters of the model.

Finally, we evaluated the choice of a model itself when considering different kinds of data points. This assignment helped us have a deeper understanding of how the properties of an estimator behave to different changes in the model. It also helped us to critically assess the choice of a model. 

\newpage

# Appendix

## Task 2A

A simulate for 2000 times in order to find the variance.

```{r}
beta = c()
for(i in 1:1000)
{
  x <- 1:n # 1, ..., 40

  set_sd = 1
  u_t = rnorm(n, mean = 0, sd = set_sd)
  y = 10 + x + u_t
  
  # solution
  estimate <- lm(y ~ x) # ignore data argument of lm as we're calling two seperate vectors
  estimate_beta = estimate$coefficients
  beta = rbind(beta, estimate_beta)
}
kable(head(beta), caption = "Regression Coefficients for Simulations when $sd=1$ (Head 6 times)")
variance_sd1 = apply(beta, 2, var)

beta = c()
for(i in 1:1000)
{
  x <- 1:n # 1, ..., 40

  set_sd = 10
  u_t = rnorm(n, mean = 0, sd = set_sd)
  y = 10 + x + u_t
  
  # solution
  estimate <- lm(y ~ x) # ignore data argument of lm as we're calling two seperate vectors
  estimate_beta = estimate$coefficients
  beta = rbind(beta, estimate_beta)
}
kable(head(beta), caption = "Regression Coefficients for Simulations when $sd=10$ (Head 6 times)")
variance_sd10 = apply(beta, 2, var)

variance = rbind(variance_sd1, variance_sd10)
kable(variance, caption = "variances for the two regression coefficients", digits = 4)
```

\newpage

## Task 2B

Detailed result of the regression for task 2B

```{r echo=FALSE}
summary(estimate_1)
kable(estimate_data_1, caption = "Results For the Regression ($sd = 1$)", digit = 3)
summary(estimate_2)
kable(estimate_data_2, caption = "Results For the Regression ($sd = 10$)", digit = 3)
```



